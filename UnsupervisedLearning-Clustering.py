# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NSnVCqlMud7H-9RXi82mRio51b-YtF8B
"""

import tensorflow as tf
import numpy as np
from matplotlib import pyplot as plt

# import the dataset and split it so that we have 54000 items in the train test, 6000 in the validation set and 10000 in the test set
from sklearn.model_selection import train_test_split
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_val, x_train, y_val, y_train  = train_test_split(x_train, y_train, test_size=0.9, random_state=1)

# run a PCA, preserving 95% of variance
from sklearn.decomposition import PCA
pca = PCA(n_components = 0.95, whiten=True)
# reshape the train set from a 3D to a 2D array
pca_created_data = pca.fit_transform(x_train.reshape(x_train.shape[0], (x_train.shape[1]*x_train.shape[2])))

# apply the existing PCA transform on validation set
pca_validation_set = pca.transform(x_val.reshape(x_val.shape[0],(x_val.shape[1]*x_val.shape[2])))

# apply inverse transform on the validation set
inversed_pca_val_set = pca.inverse_transform(pca_validation_set)

# print pictures for each label from the validation set before and after the pca
x_val_pca = inversed_pca_val_set.reshape(6000,28,28)
fig = plt.figure(figsize=(20,20))
for clusterIdx in range(10):
    for c, val in enumerate(x_val[y_val == clusterIdx][0:10]):
        fig.add_subplot(10, 10, 10*clusterIdx+c+1)
        plt.imshow(val.reshape((28,28)))
        plt.gray()
        plt.xticks([])
        plt.yticks([])
        plt.ylabel(f'label:{clusterIdx}')

fig = plt.figure(figsize=(20,20))
for clusterIdx in range(10):
    for c, val in enumerate(x_val_pca[y_val == clusterIdx][0:10]):
        fig.add_subplot(10, 10, 10*clusterIdx+c+1)
        plt.imshow(val.reshape((28,28)))
        plt.gray()
        plt.xticks([])
        plt.yticks([])
        plt.ylabel(f'label:{clusterIdx}')

# apply the existing PCA transformation on the test set
pca_test_set = pca.transform(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])))

# the performance evaluation function that will be used in the next step
from sklearn import metrics
def performance_score(input_values, cluster_indexes,true_labels):
    try:
        silh_score = metrics.silhouette_score(input_values, cluster_indexes)
        print(' .. Silhouette Coefficient score is {:.2f}'.format(silh_score))
        #-1: incorrect, 0: overlapping, +1: highly dense clusts
    except:
        print(' .. Warning: could not calculate Silhouette Coefficient score.')
        silh_score = -999

    try:
        ch_score =\
        metrics.calinski_harabasz_score(input_values, cluster_indexes)
        print(' .. Calinski-Harabasz Index score is {:.2f}'.format(ch_score))
        #Higher the value better the clusters
    except:
        print(' .. Warning: could not calculate Calinski-Harabasz Index score.')
        ch_score = -999

    try:
        db_score = metrics.davies_bouldin_score(input_values, cluster_indexes)
        print(' .. Davies-Bouldin Index score is {:.2f}'.format(db_score))
        #0: Lowest possible value, good partitioning.
    except:
        print(' .. Warning: could not calculate Davies-Bouldin Index score.')
        db_score = -999

    try:
        fm_score = metrics.fowlkes_mallows_score(true_labels, cluster_indexes)
        print(' .. Fowlkes-Mallows Index score is {:.2f}'.format(fm_score))
        #Ranges from 0 to 1. A high value indicates a good similarity between two clusters.
    except:
        print(' .. Warning: could not calculate Davies-Bouldin Index score.')
        db_score = -999

    return silh_score, ch_score, db_score, fm_score

# we run the follwing procedures for 3 to 12 clusters:
  # apply K-Means, Mini Batch K-means and Gaussian Mixture on the test data before and after PCA transformation
  # calculate the Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index and Fowlkes-Mallows Index scores
  # get a random image and compare the orignal labels to the ones calculated from the clustering algorithms

from sklearn.cluster import KMeans
from sklearn.cluster import MiniBatchKMeans
from sklearn.mixture import GaussianMixture
from numpy.random import randint
seed = 7777

for numOfClust in range (3,12):
  print(f"Currently testing {numOfClust} number of clusters:")

  # K-Means
  km = KMeans(n_clusters=numOfClust,random_state = seed)
  km.fit(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])))
  clusterLabels_kmeans = km.labels_
  print("K-means on the original test set:")
  kmeans_silh_score, kmeans_ch_score, kmeans_db_score, kmeans_fm_score  = performance_score(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])), clusterLabels_kmeans, y_test)

  km_pca = KMeans(n_clusters=numOfClust,random_state = seed)
  km_pca.fit(pca_test_set)
  clusterLabels_kmeans_pca = km_pca.labels_
  print("K-means on the PCA transformed test set:")
  kmeans_silh_score, kmeans_ch_score, kmeans_db_score, kmeans_fm_score  = performance_score(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])), clusterLabels_kmeans_pca, y_test)

  # Mini Batch K-Means
  mbkm = MiniBatchKMeans(init = "k-means++", n_clusters = numOfClust,random_state = seed)
  mbkm.fit(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])))
  clusterLabels_mkbm = mbkm.labels_
  print("Mini Batch K-means on the original test set:")
  mbk_silh_score, mbk_ch_score, mbk_db_score, mbk_fm_score  = performance_score(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])), clusterLabels_mkbm, y_test)

  mbkm_pca = MiniBatchKMeans(init = "k-means++", n_clusters = numOfClust,random_state = seed)
  mbkm_pca.fit(pca_test_set)
  clusterLabels_mkbm_pca = mbkm_pca.labels_
  print("Mini Batch K-means on the PCA transformed test set:")
  mbk_pca_silh_score, mbk_pca_ch_score, mbk_pca_db_score, mbk_pca_fm_score = performance_score(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])), clusterLabels_mkbm_pca,y_test)

  # Gaussian Mixture
  gm = GaussianMixture(n_components=numOfClust,random_state = seed)
  gm.fit(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])))
  gm_labels = gm.predict(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])))
  print("Gaussian Mixture K-means on the original test set:")
  gm_silh_score, gm_ch_score, gm_db_score, gm_fm_score  = performance_score(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])), gm_labels, y_test)

  gm_pca = GaussianMixture(n_components=numOfClust,random_state = seed)
  gm_pca.fit(pca_test_set)
  gm_pca_labels = gm_pca.predict(pca_test_set)
  print("Gaussian Mixture on the PCA transformed test set:")
  gm_pca_silh_score, gm_pca_ch_score, gm_pca_db_score, gm_pca_fm_score = performance_score(x_test.reshape(x_test.shape[0],(x_test.shape[1]*x_test.shape[2])), gm_pca_labels,y_test)

  # get a random image and compare the labels
  rand_image = np.random.randint(1,len(y_test))
  image = x_test[rand_image]
  label_original = y_test[rand_image]
  label_kmeans = clusterLabels_kmeans[rand_image]
  label_kmeans_pca = clusterLabels_kmeans_pca[rand_image]
  label_mkbm = clusterLabels_mkbm[rand_image]
  label_mkbm_pca = clusterLabels_mkbm_pca[rand_image]
  label_gm = gm_labels[rand_image]
  label_gm_pca = gm_pca_labels[rand_image]
  print(f"Labels of image {rand_image} from the test set:")
  print(f"--Original label:{label_original}\n--Kmeans label:{label_kmeans}\n--Kmeans label pca:{label_kmeans_pca}\n--Mini Batch Kmeans label:{label_mkbm}\n--Mini Batch Kmeans label pca:{label_mkbm_pca}\n--Gaussian Mixture Kmeans label:{label_gm}\n--Gaussian Mixture Kmeans label pca:{label_gm_pca}\n")